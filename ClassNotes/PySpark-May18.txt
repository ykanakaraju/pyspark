
 Agenda (PySpark)
 -----------------
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming
	

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/pyspark

 ============================================================


   Spark
   -----
       	-> Spark is a unified in-memory distributed computing framework.
	-> Used for big data analytics

	-> Spark is written in "Scala" language.

	-> Spark is a polyglot
		Supports  Scala, Java, Python, R


   Spark Unified Framework
   -----------------------

	Spark provided a set of consistent APIs based on the same execution engine to process different
	analytics workloads. 

		Batch Processing of unstructured data	=> Spark Core API (RDDs)
		Batch Processing of structured data	=> Spark SQL
		Stream processing (real time)		=> Structured Streaming, DStreams API
		Predictive analytics (ML)		=> Spark MLLib
		Graph parallel computations		=> Spark GraphX



    Getting started with Spark
    --------------------------
	
     1. Working in your vLab

	    -> Follow the instructions document and go to Windows server.
	    -> Windows server desktop has:
			1. A word document with user credentials and other instruction
			2. An icon of "Oracle VM virtualbox" to launch the lab VM

	    -> Click on the "Oracle VM virtualbox" to launch the lab 

		  -> Open a terminal and type "pyspark"
		  -> Launch Spyder IDE to write programs.


     2. Setting up your own PySpark environment on a local machine.

	-> Install "Anaconda Navigator" 
	-> Open an  Ananconda Terminal and pip install pyspark.

	-> If pip install is not working for you, you can use the instructions mentioned in the shared document.
	   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf


     3. Signup to free account - Databricks community edition

		Signup URL: https://www.databricks.com/try-databricks
		=> Make sure you clikc on "Databrick Comminity Edition" option

		Signin: https://community.cloud.databricks.com/login.html


    Spark Architecture
    ------------------

	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver.


    RDD (Resilient Distributed Datasets)
    ------------------------------------
  
    -> RDD is the fundamental data abstraction of Spark.

    -> RDD is a collection of distributed in-memory partitions.
	-> A partition is a collection of objects. 

    -> RDDs are lazily evaluated
	-> Transformations only cause the lineage DAGs to be created. Does not cause executition	
	-> Action commands causes execution.

    -> RDDs are immmutable


   Creating RDDs
   -------------
	
    Three ways:

	1. Create an RDD from external data file

		rddFile = sc.textFile( "E:\\Spark\\wordcount.txt", 4 )

	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize( range(1, 100), 3 )

	3. By applying transformations on existing RDDs

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   RDD Operations
   --------------

    Two operations:

	1. Transformations
		-> Only create RDD linage DAGS
		-> Does not cause execution

	2. Actions
		-> Trigger execution of an RDD
		-> Generates output.

   RDD Lineage DAG
   ---------------

	rddFile = sc.textFile( "E:\\Spark\\wordcount.txt", 4 )
	Lineage of rddFile :  (4) rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split(" "))
	Lineage of rddWords :  (4) rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage of rddPairs :  (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)		
	Lineage of rddWc :  (4) rddWc ->  rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile
	

   Types of Tansformations
   -----------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD


   RDD Persistence
   ---------------

	rdd1 = sc.textFile(<filePath>, 10)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist()   -----------> instruction of Spark to save rdd6 partitions. 
	rdd7 = rdd6.t7(...)

	rdd6.collect()

	lineage of rdd6: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[sc.textFile, t3, t5, t6] -> collect

        rdd7.collect()

	lineage of rdd7: rdd7 -> rdd6.t7
		[t7] -> collect

	rdd6.unpersist()



	Storage Levels
        ---------------	
	1. MEMORY_ONLY		-> default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	-> Disk Memory Serialized 1x Replicated
	3. DISK_MEMORY		-> Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	-> Memory Serialized 2x Replicated
	5. MEMORY_AND_DISK_2	-> Disk Memory Serialized 2x Replicated

	Commands
	--------
	rdd1.cache()    				-> in-memory persistence
	rdd1.persist()	 				-> in-memory persistence
	rdd1.persist( StorageLevel.MEMORY_AND_DISK )

	rdd1.unpersist()


   RDD Transformations
   -------------------

   1. map		P: U -> V
			object to object transformation
			input RDD: N objects, output RDD: N objects

		rdd2 = rdd1.map(lambda x: x > 7)

   2. filter		P: U -> Boolean
			only those objects of the input RDD for which the function returns True will be
			there in the output RDD.
			input RDD: N objects, output RDD: <= N objects

   3. glom		P: None
			returns one list object per input partition with all the objects of the partition'
			input RDD: N objects, output RDD: = # of partitions

		rdd1		 rdd2 = rdd1.glom()
		P0: 3,2,5,6,4,6,7 -> glom -> P0: [3,2,5,6,4,6,7]
		P1: 5,7,5,6,8,9,0 -> glom -> P1: [5,7,5,6,8,9,0]
		P2: 7,0,3,6,1,1,3 -> glom -> P2: [7,0,3,6,1,1,3]

		rdd1.count() = 21 (int)	     rdd2.count() = 3 (list)

   4. flatMap		P: U -> Iterable[V]
			Flattens the iterables produced by the function.
			input RDD: N objects, output RDD: = > N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


   5. mapPartitions		P: Iterable[U] -> Iterable[V]
				partition to partition transformation

		rdd1	      rdd2 = rdd1.mapPartitions(lambda x: [sum(x)] )
		P0: 3,2,5,6,4,6,7 -> mapPartitions -> P0: 33
		P1: 5,7,5,6,8,9,0 -> mapPartitions -> P1: 40
		P2: 7,0,3,6,1,1,3 -> mapPartitions -> P2: 21

		rdd1.mapPartitions(lambda p: map(lambda x: x*2, p)).glom().collect()
		

   6. mapPartitionsWithIndex	p: Int, Iterable[U] -> Iterable[V]
				partition to partition transformation
				We get the partition-index as additional function parameter.   

		rdd1.mapPartitionsWithIndex(lambda i,p: [(i, len(list(p)))] ).collect()
		rdd1.mapPartitionsWithIndex(lambda i, p: [(i, list(p))]).filter(lambda p: p[0] == 1).collect()


   7. distinct			P: None, Optional: numPartitions
				Returns unique objects of the RDD

		rddWords.distinct().collect()


    Spark Application Execution Flow
    ---------------------------------
	Spark Application (SparkContext)  (-> PySpark, Spark-Submit)	
	    -> Job  (Each action command)
		-> Stages (1 or more stages)
		    -> Tasks (every wide transformation causes a stage transition)
			-> Transformations

	


   
    

    














  













