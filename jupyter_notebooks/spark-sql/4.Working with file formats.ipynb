{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./Setup.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_path = \"E:\\\\PySpark\\\\data\\\\flight-data\\json\\\\2015-summary.json\"\n",
    "#json_path = \"E:\\\\PySpark\\\\data\\\\flight-data\\json\\\\*-summary.json\"\n",
    "json_path = \"E:\\\\PySpark\\\\data\\\\flight-data\\json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = spark.read.json(json_path)\n",
    "json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get('spark.sql.files.maxPartitionBytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df.show()\n",
    "#json_df.show(30)\n",
    "#json_df.show(30, False)\n",
    "#json_df.show(3, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatic Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = spark.read.json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(json_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = StructType(\n",
    "    [\n",
    "        StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "        StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "        StructField(\"count\", IntegerType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema_2 = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count BIGINT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df_1 = spark.read.schema(json_schema_2).json(json_path)\n",
    "#json_df_1 = spark.read.json(json_path, schema=json_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df_2 = json_df_1.where(\"count > 200\")\n",
    "json_df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df_2.write.json(\"E:\\\\PySpark\\\\output\\\\json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df_2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using multi-line JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_line_json_path = \"E:\\\\PySpark\\\\data\\\\users_multiline.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_line_json_df = spark.read.json(multi_line_json_path, multiLine=True)\n",
    "multi_line_json_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using nested JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_json_path = \"E:\\\\PySpark\\\\data\\\\users-nested.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_json_df = spark.read.json(nested_json_path)\n",
    "nested_json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_json_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nested_json_df.select(\"userid\", \"name\", \"address.city\", \"address.state\").show()\n",
    "nested_json_df.select(\"userid\", \"name\", \"address.*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"E:\\\\PySpark\\\\data\\\\flight-data\\\\parquet\\\\2010-summary.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parquet_df = spark.read.format(\"parquet\").load(parquet_path)\n",
    "parquet_df = spark.read.parquet(parquet_path)\n",
    "parquet_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_df_2 = parquet_df.where(\"count > 100 and DEST_COUNTRY_NAME = 'United States'\")\n",
    "parquet_df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_df_2.write.mode(\"overwrite\").parquet(\"E:\\\\PySpark\\\\output\\\\parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_df_2\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .parquet(\"E:\\\\PySpark\\\\output\\\\parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_df_2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORC format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orc_path = \"E:\\\\PySpark\\\\data\\\\flight-data\\\\orc\\\\2010-summary.orc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#orc_df = spark.read.format(\"orc\").load(orc_path)\n",
    "orc_df = spark.read.orc(orc_path)\n",
    "orc_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orc_path_2 = orc_df.where(\"count > 100\")\n",
    "orc_path_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#orc_path_2.write.format(\"orc\").save(\"E:\\\\PySpark\\\\output\\\\orc\")\n",
    "orc_path_2.write.orc(\"E:\\\\PySpark\\\\output\\\\orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV format\n",
    "\n",
    "    - Represents any delimited text file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_path = \"E:\\\\PySpark\\\\data\\\\flight-data\\\\csv\\\\2015-summary.csv\"\n",
    "csv_path = \"E:\\\\PySpark\\\\data\\\\flight-data\\\\csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_schema = \"destination STRING, origin STRING, count BIGINT\"\n",
    "#csv_schema = \"destination STRING, origin STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#csv_df = spark.read.csv(csv_path).toDF(\"destination\", \"origin\", \"count\")\n",
    "#csv_df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "csv_df = spark.read.csv(csv_path, header=True, schema=csv_schema)    #programmatic schema\n",
    "\n",
    "csv_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df_2 = csv_df.where(\"count > 1000\")\n",
    "csv_df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_df_2.write.csv(\"E:\\\\PySpark\\\\output\\\\csv\", header=True, mode=\"overwrite\")\n",
    "#csv_df_2.write.csv(\"E:\\\\PySpark\\\\output\\\\csv\", header=True, mode=\"overwrite\", sep=\"\\t\")\n",
    "csv_df_2.write.csv(\"E:\\\\PySpark\\\\output\\\\csv\", header=True, mode=\"overwrite\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df_2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df_2 = spark.read.csv(\"E:\\\\PySpark\\\\output\\\\csv\", header=True, sep=\";\")\n",
    "csv_df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = \"E:\\\\PySpark\\\\data\\\\wordcount.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = spark.read.text(text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|value                                         |\n",
      "+----------------------------------------------+\n",
      "|spark is a general purpose execution framework|\n",
      "|spark can run on hadoop                       |\n",
      "|scala is preferred language for spark         |\n",
      "|spark also supports java and python           |\n",
      "|spark is a general purpose execution framework|\n",
      "|spark can run on hadoop                       |\n",
      "|scala is preferred language for spark         |\n",
      "|spark also supports java and python           |\n",
      "|python spark scala java pyspark hadoop        |\n",
      "|spark python spark rdd rdd rdd sql spark      |\n",
      "|pythom machine learning spark sql rdd rdd     |\n",
      "|spark is a general purpose execution framework|\n",
      "|spark can run on hadoop                       |\n",
      "|scala is preferred language for spark         |\n",
      "|spark also supports java and python           |\n",
      "|spark is a general purpose execution framework|\n",
      "|spark can run on hadoop                       |\n",
      "|scala is preferred language for spark         |\n",
      "|spark also supports java and python           |\n",
      "|python spark scala java pyspark hadoop        |\n",
      "|spark python spark rdd rdd rdd sql spark      |\n",
      "|pythom machine learning spark sql rdd rdd     |\n",
      "|spark is a general purpose execution framework|\n",
      "|spark can run on hadoop                       |\n",
      "|scala is preferred language for spark         |\n",
      "|spark also supports java and python           |\n",
      "|spark is a general purpose execution framework|\n",
      "|spark can run on hadoop                       |\n",
      "|scala is preferred language for spark         |\n",
      "|spark also supports java and python           |\n",
      "|python spark scala java pyspark hadoop        |\n",
      "|spark python spark rdd rdd rdd sql spark      |\n",
      "|pythom machine learning spark sql rdd rdd     |\n",
      "+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|word     |count|\n",
      "+---------+-----+\n",
      "|spark    |39   |\n",
      "|is       |12   |\n",
      "|a        |6    |\n",
      "|general  |6    |\n",
      "|purpose  |6    |\n",
      "|execution|6    |\n",
      "|framework|6    |\n",
      "|can      |6    |\n",
      "|run      |6    |\n",
      "|on       |6    |\n",
      "|hadoop   |9    |\n",
      "|scala    |9    |\n",
      "|preferred|6    |\n",
      "|language |6    |\n",
      "|for      |6    |\n",
      "|also     |6    |\n",
      "|supports |6    |\n",
      "|java     |9    |\n",
      "|and      |6    |\n",
      "|python   |12   |\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordcount_df = text_df \\\n",
    "    .select(explode(split(\"value\", \" \")).alias(\"word\")) \\\n",
    "    .groupBy(\"word\") \\\n",
    "    .count()\n",
    "    \n",
    "wordcount_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- split(value,  , -1): array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordcount_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount_df.select(\"word\").write.text(\"E:\\\\PySpark\\\\output\\\\text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount_df \\\n",
    "    .withColumn(\"word\", concat( col(\"word\"), lit(\",\"), col(\"count\"))) \\\n",
    "    .select(\"word\") \\\n",
    "    .write.text(\"E:\\\\PySpark\\\\output\\\\text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
