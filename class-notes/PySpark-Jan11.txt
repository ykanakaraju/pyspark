
 Agenda (PySpark)
 ----------------
   Spark - Basics & Architecture
   Spark Core API Basics
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes
	=> Databricks Notebooks
        => Github: https://github.com/ykanakaraju/pyspark

  
  Spark
  -----   
    => Spark is in-memory distributed computing framework for big data analytics.

    	in-memory: ability to persist intermediate results and subsequent operations
		   can directly work on these persisted intermediate results. 

    => Spark is written in Scala programming language.

    => Spark is polyglot
	 -> Scala, Java, Python, R (and SQL)

    => Spark is a unified analytics framework

    => Spark can run on multiple cluster managers
	  local, spark standalone scheduler, YARN, Mesos, Kubernetes
  
  
  Spark Unified Framework
  -----------------------

	Spark provides a consistent set of APIs for performing different analytics workloads
        using the same execution engine and some well defined data abstractions and operations.

   	Batch Analytics of unstructured data	-> Spark Core API (low-level api)
	Batch Analytics of structured data 	-> Spark SQL
	Streaming Analytics (real time)		-> Spark Streaming, Structured Streaming
	Predictive analytics (ML)		-> Spark MLLib
	Graph parallel computations  		-> Spark GraphX


   Getting started with Spark
   --------------------------

	1. Setting up local dev environment on your personal machine.

		-> Install Anaconda distribution for Python. 
		   URL: https://www.anaconda.com/download

		-> Follow the instructions given in the shared document 
		   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

	* 2. Signup to Databricks Community Edition (free edition)
 		
		Signup: https://www.databricks.com/try-databricks

			Screen 1: Fill up the details with valid email address
			Screen 2: Click on "Get started with Community Edition" link (Not the 'Continue' button)

		Login: https://community.cloud.databricks.com/login.html

		Downloading a file from Databricks
		----------------------------------
		/FileStore/<FILEPATH>
		https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439

		Example:
		file path to be downloaded: dbfs:/FileStore/ctsdatasets/output/wc/part-00000
		https://community.cloud.databricks.com/files/ctsdatasets/output/wc/part-00000?o=1072576993312365

  
  Spark Architecture
  ------------------

    	1. Cluster Manager (CM)
		-> Applications are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the 'SparkContext' object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		-> Where to run the driver process
		1. Client : default, driver runs on the client. 
		2. Cluster: driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks run the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 


   RDD (Resilient Distributed Dataset)
   -----------------------------------

	-> RDD is the fundamental data abstraction of Spark

	-> RDD is a collection of distributed in-memory partitions.
	    -> Each partition is a collection of objects of some type.

	-> RDDs are immutable

	-> RDDs are lazily evaluated
	   -> Transformations does not cause execution
	   -> Action commands trigger execution.	
 
  Creating RDDs
  -------------	
    Three ways:

	1. Create an RDD from external data files.

		rddFile = sc.textFile( <filePath> , 4 )

		default Number of partitions: sc.defaultMinPartition
		  sc.defaultMinPartition = 2, if number of cores >= 2
					   1, otherwise

	2. Create an RDD from programmatic data

		rdd1 = sc.parallelize([2,4,3,1,5,6,7,8,6,8,9,0,7,5,4,6,8], 2)
		
		default Number of partitions: sc.defaultParallelism
		sc.defaultParallelism = number of CPU cores allocated.


	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*10)


  RDD Operations
  --------------

    Two types of operations

	1. Transformations
		-> Transformations return RDDs
		-> Transformations does not cause execution of RDDs
		-> Cause lineage DAGs to be created

	2. Actions
		-> Triggers execution of RDDs
		-> Produces output by sending a Job to the cluster


  RDD Lineage DAG
  ---------------
  Driver maintains a Lineage DAG for every RDD
  Lineage DAG is a heirarchy of dependencies of RDDs all the way starting from the very first RDD	
  Lineage DAG is a logical plan on how to create the RDD.

   	rddFile = sc.textFile("E:\Spark\wordcount.txt", 4)
	   Lineage of rddFile -> (4) rddFile -> sc.textFile on E:\Spark\wordcount.txt
	
	rddWords = rddFile.flatMap(lambda x: x.split())
	   Lineage of rddWords -> (4) rddWords -> rddFile.flatMap -> sc.textFile

	rddPairs = rddWords.map(lambda x: (x, 1))
	   Lineage of rddPairs -> (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	   Lineage of rddWc -> (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile


  Types of RDD Transformation
  ---------------------------

    Two types:

	 1. Narrow Transformations
           -> Narrow transformations are those, where the computation of each partition depends ONLY
              on its input partition.
           -> There is no shuffling of data.
           -> Simple and efficient


      	2. Wide Transformations
           -> In wide transformations, the computation of a single partition depends on many
              partitions of its input RDD.
           -> Data shuffle across partitions will happen.
           -> Complex and expensive


  Spark execution flow
  --------------------

	Application (created by a SparkContext)
	|
	|=> Jobs (each action command lauches a job)
	    |
	    |=> Stages (one or more stages; each wide transformation create a new stage)
		|
		|=> Tasks ( set of transformations that can run in partition)
		    |
  		    |=> Transformations (that can run in parallel)
 

  RDD Transformations
  -------------------
  
  1. map		P: U -> V
			Object to object transformation
			Input RDD: N objects, Output RDD: N objects					

  		rdd2 = rddFile.map(lambda x : x.split())


  2. filter		P: U -> Boolean
			Filters the objects based on the function
			Input RDD: N objects, Output RDD: <= N objects	

		rddFile.filter(lambda x: len(x.split()) > 8 ).collect()

  3. glom		P: None
			Return one list object per partition with all the objects of the partition

		rdd1			rdd2 = rdd1.glom()

		P0: 3,2,1,3,2,4 -> glom -> P0: [3,2,1,3,2,4]
		P1: 5,4,3,6,7,8 -> glom -> P1: [5,4,3,6,7,8]
		P2: 3,2,6,5,7,0 -> glom -> P2: [3,2,6,5,7,0]

		rdd1.count() = 18 (int)	   rdd2.count() = 3 (list)
	
	
  4. flatMap		P: U -> Iterable[V]
			flattens the iterabels generated by the function
 			Input RDD: N objects, Output RDD: >= N objects	


		rddWords = rddFile.flatMap(lambda x: x.split())


  5. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p) ).collect()
		rdd1.mapPartitions(lambda p: [sum(p)] ).collect()


  6. mapPartitionsWithIndex   P: Int, Iterable[U] -> Iterable[V]
			Similar to mapPartitions, but we get the partition index as an additional parameter.

		rdd1 \
		.mapPartitionsWithIndex(lambda i,p: map(lambda x: (i, x), p)) \
		.filter(lambda x: x[0] == 1) \
		.map(lambda x: x[1]) \
		.collect()


  7. distinct	P: None, Optional: numPartitions
		Returns distinct objects of the RDD.

		rddWords.distinct().collect()



  Types of RDDs
  -------------
	Generic RDDs: RDD[U]            
	Pair RDD: RDD[(K, V)]


  8. mapValues		P: U => V
			Applied only on Pair RDD
			Applies the function only to the 'value' part of the key-value pairs

		rddPairs.mapValues(lambda x: x*10).collect()
		-> In the above fn, x represents only the 'value' part of key-value pairs




 

















