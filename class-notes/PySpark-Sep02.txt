
 Agenda (PySpark) - (9 sessions * 4 sessions)
 --------------------------------------------
   Spark - Basics & Architecture
   Spark Core API Basics
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming
   Spark Performance Tuning & Optimization


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Databricks Notebooks 
	=> Class Notes
        => Github: https://github.com/ykanakaraju/pyspark


 
  Spark
  -----
    => Spark is in-memory distributed computing framework for big data analytics.

	in-memory: ability to persist intermediate results in RAM and subsequent operations
		   can directly work on these persisted intermediate results. 

    => Spark is written in Scala programming language

    => Spark is a polyglot
	-> Scala, Java, Python, R (and SQL)

    => Spark applications can run on:
	-> local, Spark standalone, YARN, Mesos, Kubernetes

    => Spark is unified analytics framework.


  Spark Unified Framework
  -----------------------
    Spark provides a consistent set of APIs for performing different analytics workloads
    using the same execution engine and some well defined data abstractions and operations.

	Batch Analytics			-> Spark SQL
	Streaming Analytics (real time)	-> Structured Streaming, DStreams API
	Predictive analytics (ML)	-> Spark MLLib  (mllib & ml)
	Graph parallel computations  	-> Spark GraphX


   Spark Architecture
   ------------------

	1. Cluster Manager (CM)
		-> Applications are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the 'SparkContext' object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		-> Where to run the driver process
		1. Client : default, driver runs on the client. 
		2. Cluster: driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> Receives the tasks from the Driver
		-> All tasks run the same code but on different partitions of the data
		-> The status of tasks are reported to the driver. 



  Getting started with Spark on Databricks
  ----------------------------------------   
   ** Databricks Community Edition (free edition)
 		
	Signup: https://www.databricks.com/try-databricks
		Screen 1: Fill up the details with valid email address
		Screen 2: Click on "Get started with Community Edition" link (Not the 'Continue' button)

	Login: https://community.cloud.databricks.com/login.html

	Downloading a file from Databricks
	----------------------------------
		/FileStore/<FILEPATH>
		https://community.cloud.databricks.com/files/<FILEPATH>?o=1072576993312365

		Example:
		dbfs:/FileStore/output/wc/part-00000
		https://community.cloud.databricks.com/files/output/wc/part-00000?o=1072576993312365

		dbfs:/FileStore/output/wordcount1/part-00000
		https://community.cloud.databricks.com/files/output/wordcount1/part-00000?o=1072576993312365


 	Enabling DBFS File browser
	--------------------------
	<your account (top-right)> -> Settings -> Advanced -> Other -> DBFS File Browser (enable it)


   RDD (Resilient Distributed Dataset)
   -----------------------------------

	-> RDD is the fundamental data abstraction of Spark

	-> RDD is a collection of distributed in-memory partitions.
	    -> Each partition is a collection of objects of some type.

	-> RDDs are immutable

	-> RDDs are lazily evaluated
	   -> Transformations does not cause execution.
	   -> Action commands trigger execution.   


   Creating RDDs
   -------------

    Three ways:

	1. Creating an RDD from external data file

		rdd1 = sc.textFile(<dataPath>, 4)

		default Number of partitions: sc.defaultMinPartition
		  sc.defaultMinPartition = 2, if number of cores >= 2
					   1, otherwise

	2. Creating an RDD from programmatic data

		rdd1 = sc.parallelize([2,1,3,2,4,3,5,4,6,7,5,6,7,6,8,8,9,0], 2)

		default Number of partitions: sc.defaultParallelism
		sc.defaultParallelism = number of CPU cores allocated.

	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*10)

		
   RDD Operations
   --------------

    Two types of operations

	1. Transformations
		-> Transformations return RDDs
		-> Transformations does not cause execution of RDDs
		-> Cause lineage DAGs to be created

	2. Actions
		-> Triggers execution of RDDs
		-> Produces output by sending a Job to the cluster


   RDD Lineage DAG
   ---------------
    Driver maintains a Lineage DAG for every RDD.
    Lineage DAG is a heirarchy of dependencies of RDDs all the way starting from the very first RDD.	
    Lineage DAG is a logical plan on how to create the RDD.

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage DAG : (4) rddFile -> sc.textFile

	rddWords = rddFile.flatMap(lambda x: x.split())
	Lineage DAG : (4) rddWords -> rddFile.flatMap -> sc.textFile
	
	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage DAG : (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	Lineage DAG : (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile



  Types of Transformations
  ------------------------
    Two types:

	1. Narrow Transformations
           -> Narrow transformations are those, where the computation of each partition depends ONLY
              on its input partition.
           -> There is no shuffling of data.
           -> Simple and efficient

      	2. Wide Transformations
           -> In wide transformations, the computation of a single partition depends on many
              partitions of its input RDD.
           -> Data shuffle across partitions will happen.
           -> Complex and expensive


  RDD Transformations
  -------------------
  
  1. map		P: U -> V
			Object to object transformation
			Input RDD: N objects, Output RDD: N objects					

  		rdd2 = rddFile.map(lambda x : x.split())


  2. filter		P: U -> Boolean
			Filters the objects based on the function
			Input RDD: N objects, Output RDD: <= N objects	

		rddFile.filter(lambda x: len(x.split()) > 8 ).collect()


  3. glom		P: None
			Return one list object per partition with all the objects of the partition
			Input RDD: N objects, Output RDD: number of objects = number of partitions

		rdd1	    rdd2 = rdd1.glom()

		P0: 3,4,2,4,6  -> glom -> P0: [3,4,2,4,6]
		P1: 4,6,7,8,6  -> glom -> P1: [4,6,7,8,6] 
		P2: 5,0,7,9,1  -> glom -> P2: [5,0,7,9,1] 

  4. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p) ).collect()
		rdd1.mapPartitions(lambda p: [sum(p)] ).collect()



   5. flatMap		P: U -> Iterable[V]
			flattens the iterables generated by the function
 			Input RDD: N objects, Output RDD: >= N objects	

		rddWords = rddFile.flatMap(lambda x: x.split())


      Types of RDDs
      -------------
	Generic RDDs: 	RDD[U]            
	Pair RDD: 	RDD[(K, V)]

 
   6. mapValues		P: U => V
			Applied only on Pair RDDs
			Applies the function only to the 'value' part of the key-value pairs

		rddPairs.mapValues(lambda x: x*10).collect()
		-> In the above fn, x represents only the 'value' part of key-value pairs


   7. distinct		P: None, Optional: numPartitions
			Returns distinct objects of the RDD.

			rddWords.distinct().collect()
			rddWords.distinct(4).collect()


   8. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Sorts the RDDs based on the function output.

		rdd1.sortBy(lambda x: x%3).glom().collect()
		rdd1.sortBy(lambda x: x%3, False).glom().collect()
		rdd1.sortBy(lambda x: x%3, True, 2).glom().collect()
	 

    9. groupBy		P: U -> V, Optional: numPartitions

			Returns a Pair-RDD where:
			    key: Each unique value of the function output
		            value: ResultIterable. Grouped objects of the RDD that produced the key 


    10. partitionBy	P: numPartitions, Optional: partition-function (default: hash)
			Applied only on Pair RDDs
			Controls which keys go to which partition based on partition-function applied to keys.
           
  		rdd4 = rdd1.map(lambda x: (x, 1)).partitionBy(3).map(lambda x: x[0])


  	..ByKey Transformations
  	-----------------------
	=> Are wide transformations
	=> Applied on Pair RDDs only


    11. sortByKey	P: None, Optional: ascending (True/False), numPartitions
			Sorts the data based on the keys.

			rddPairs.sortByKey().glom().collect()
			rddPairs.sortByKey(False).glom().collect()
			rddPairs.sortByKey(False, 3).glom().collect()	


    12. groupByKey	P: None, Optional: numPartitions
			Returns a Pair RDD where:
				key: Unique keys of the RDD
				value: ResultIterable. Grouped values

			CAUTION: avoid groupByKey if possible. 

		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 1) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.groupByKey() \
        		.mapValues(sum) \
        		.sortBy(lambda x: x[1], False) 


   13. reduceByKey	P: (U, U) -> U,  Optional: numPartitions
			Reduce all the values of each unique key by iterativly applying the reduce function - first, on
			each partition, and then, across the reduced values of all partitions. 
			
		rddWc = sc.textFile("E:\\Spark\\wordcount.txt", 1) \
        		.flatMap(lambda x: x.split()) \
        		.map(lambda x: (x, 1)) \
        		.reduceByKey(lambda x, y: x + y) \
        		.sortBy(lambda x: x[1], False)

   14. repartition	P: numPartitions
			Is used to increase or decrease the number of output partitions	
			Global shuffle	
			
		rdd2 = rdd1.repartition(5)


   15. coalesce		P: numPartitions
			Is used to only decrease the number of output partitions	
			Partition merging

		rdd2 = rdd1.coalesce(5)


		Recommendations
		---------------
		-> The size of each partition should be 128 MB (or less)
		-> The number of partitions should be a multiple of number of cores
		-> The number of cores per executor should be 5 (if you are using YARN)

  Actions
  -------

  1. collect

  2. count

  3. saveAsTextFile

  4. reduce		P: (U, U) -> U
			Reduces an entire RDD to one object by iterativly applying the function - first, on
			each partition, and then, across the reduced values of all partitions. 

		rdd1
		
		P0: 3,2,1,7,5  -> reduce -> -12 -> reduce -> -4
		P1: 6,8,4,2,1  -> reduce -> -9
		P2: 9,4,3,1,0  -> reduce -> 1

		rdd1.reduce( lambda x, y: x - y )


   RDD Persistence
   ---------------
   	rdd1 = sc.textFile(<data_path>, 10)
	rdd2 = rdd1.t2(...)
	rdd3 = rdd1.t3(...)
	rdd4 = rdd3.t4(...)
	rdd5 = rdd3.t5(...)
	rdd6 = rdd5.t6(...)
	rdd6.persist()       --> instruction to Spark to save rdd6 partitions
	rdd7 = rdd6.t7(...)

	rdd6.collect()
	Lineage of rdd6 => (10) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	tasks (10) -> [sc.textFile, t3, t5, t6] -> collected

	rdd7.collect()
	Lineage of rdd7 => (10) rdd7 -> rdd6.t7
	tasks (10) -> [t7] -> collected

	rdd6.unpersist()


        Storage Levels
        --------------
	MEMORY_ONLY		=> default, Memory Serialized 1x replicated
	MEMORY_AND_DISK		=> Disk Memory Serialized 1x replicated
	DISK_ONLY		=> Disk Serialized 1x replicated
	MEMORY_ONLY_2		=> Memory Serialized 2x replicated	
	MEMORY_AND_DISK_2	=> Disk Memory Serialized 2x replicated	



	Commands
	--------

	rdd1.cache()     			-> in-memory persistence
	rdd1.persist()   			-> in-memory persistence
	rdd1.persist( StorageLevel.DISK_ONLY )

	rdd1.unpersist()












     