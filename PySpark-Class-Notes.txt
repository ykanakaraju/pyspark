
  Agenda (PySpark) - 8 sessions of 4 hrs each
  --------------------------------------------
   Spark - Basics & Architecture
   Spark Core API Basics
	-> RDD Transformations & Actions
	-> Shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Databricks Notebooks & Jupyter Notebook 
	=> Class Notes
        => Github: https://github.com/ykanakaraju/pyspark 


  Spark
  -----

    => Spark is in-memory distributed computing framework for big data analytics.

	in-memory: ability to persist intermediate results in RAM and subsequent operations
		   can directly work on these persisted intermediate results. 

    => Spark is written in Scala programming language

    => Spark is a polyglot
	-> Scala, Java, Python, R (and SQL)

    => Spark applications can run on:
	-> local, Spark standalone, YARN, Mesos, Kubernetes

    => Spark is unified analytics framework.

  
    Spark Unified Framework
    -----------------------
    Spark provides a consistent set of APIs for performing different analytics workloads
    using the same execution engine and some well defined data abstractions and operations.

	Batch Analytics			-> Spark SQL
	Streaming Analytics (real time)	-> Structured Streaming, DStreams API
	Predictive analytics (ML)	-> Spark MLLib  (mllib & ml)
	Graph parallel computations  	-> Spark GraphX


  Getting started with Spark
  -------------------------- 

   1. Setting up local dev environment

	Ref URL: https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf  
	Spark download: https://spark.apache.org/downloads.html


   2. Databricks Free Edition (preferred)


  Setting up PySpark on Windows machine
  -------------------------------------

	1. Install Java 8 or up

		java -version
		
		https://www.oracle.com/in/java/technologies/downloads/#jdk24-windows

	2. Add JAVA_HOME environment variable

		Go to 'Edit system environemt variables' windows
		Add JAVA_HOME env. variable with the PATH were Java is installed.

	3. Download and extract Spark binaries.

		URL: https://spark.apache.org/downloads.html
		Choose the version
		Download spark: <click on this link>
		Go to the mirror site and download


		-> Extract the downloaded file in a suitable folder
		(tar -xvf <path-of-tgz file>)

	4. Add SPARK_HOME environment variable

		Go to 'Edit system environemt variables' windows
		Add SPARK_HOME env. variable with the PATH were Spark is extracted.

	5. Setup Hadoop winutils for windows 

		URL: https://github.com/cdarlint/winutils
		Download the repo
		Extract it and copy the folder to your Spark path. 

	6. Add HADOOP_HOME environment variable

		Go to 'Edit system environemt variables' windows
		Add HADOOP_HOME env. variable with the PATH were hadoop is extracted.

	7. Add the "bin" folders of the above to the PATH environment variable

		%JAVA_HOME%\bin
		%SPARK_HOME%\bin
		%HADOOP_HOME%\bin

	8. Open a command terminal and type spark-shell

		C:> spark-shell

	9. Download and install Python (3 or above)

	10. Pip install find-spark library for python

		pip install findspark


  Getting started with Spark on Databricks Free Edition
  -----------------------------------------------------

   -> Click on https://www.databricks.com/try-databricks
   -> Click on "Try Databricks" button (top-right corner)
   -> Click on "Click here" link towsrds the bottom
	** Looking for Databricks Free Edition? Click here
   -> Provide a valid email address 
   -> Enter the validation code sent to the email to login to Databricks free edition


  Spark Architecture
  ------------------

	1. Cluster Manager (CM)
		-> Applications are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the 'SparkContext' object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		  -> Where to run the driver process
		  1. Client : default, driver runs on the client. 
		  2. Cluster: driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> Receives the tasks from the Driver
		-> All tasks run the same code but on different partitions of the data
		-> The status of tasks are reported to the driver. 


  RDD (Resilient Distributed Dataset)
  -----------------------------------

	-> RDD is the fundamental data abstraction of Spark

	-> RDD is a collection of distributed in-memory partitions.
	    -> Each partition is a collection of objects of some type.

	-> RDDs are immutable

	-> RDDs are lazily evaluated
	   -> Transformations does not cause execution.
	   -> Action commands trigger execution. 	



  RDD (Resilient Distributed Dataset)
  -----------------------------------

	-> RDD is the fundamental data abstraction of Spark

	-> RDD is a collection of distributed in-memory partitions.
	    -> Each partition is a collection of objects of some type.

	-> RDDs are immutable

	-> RDDs are lazily evaluated
	   -> Transformations does not cause execution.
	   -> Action commands trigger execution. 


   Creating RDDs
   -------------
    Three ways:

	1. Creating an RDD from external data file

		rdd1 = sc.textFile(<dataPath>, 4)

		default Number of partitions: sc.defaultMinPartition
		  sc.defaultMinPartition = 2, if number of cores >= 2
					   1, otherwise

	2. Creating an RDD from programmatic data

		rdd1 = sc.parallelize([2,1,3,2,4,3,5,4,6,7,5,6,7,6,8,8,9,0], 2)

		default Number of partitions: sc.defaultParallelism
		sc.defaultParallelism = number of CPU cores allocated.

	3. By applying transformations on existing RDDs

		rdd2 = rdd1.map(lambda x: x*10)


   RDD Operations
   --------------

    Two types of operations

	1. Transformations
		-> Transformations return RDDs
		-> Transformations does not cause execution of RDDs
		-> Cause lineage DAGs to be created

	2. Actions
		-> Triggers execution of RDDs
		-> Produces output by sending a Job to the cluster


  RDD Lineage DAG
  ---------------
    Driver maintains a Lineage DAG for every RDD.
    Lineage DAG is a heirarchy of dependencies of RDDs all the way starting from the very first RDD.	
    Lineage DAG is a logical plan on how to create the RDD partitions.

	rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	Lineage DAG : (4) rddFile -> sc.textFile on E:\\Spark\\wordcount.txt

	rddWords = rddFile.flatMap(lambda x: x.split())
	Lineage DAG : (4) rddWords -> rddFile.flatMap -> sc.textFile on E:\\Spark\\wordcount.txt

	rddPairs = rddWords.map(lambda x: (x, 1))
	Lineage DAG : (4) rddPairs -> rddWords.map -> rddFile.flatMap -> sc.textFile on E:\\Spark\\wordcount.txt

	rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	Lineage DAG : (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> sc.textFile 
	

  RDD Transformations
  -------------------

  1. map		P: U -> V
			Object to object transformation
			Input RDD: N objects, Output RDD: N objects					

  		rdd2 = rddFile.map(lambda x : x.split())


   2. filter		P: U -> Boolean
			Filters the objects based on the function
			Input RDD: N objects, Output RDD: <= N objects	

		rddFile.filter(lambda x: len(x.split()) > 8 ).collect()


   3. glom		P: None
			Return one list object per partition with all the objects of the partition
			Input RDD: N objects, Output RDD: number of objects = number of partitions

		rdd1	    rdd2 = rdd1.glom()

		P0: 3,4,2,4,6  -> glom -> P0: [3,4,2,4,6]
		P1: 4,6,7,8,6  -> glom -> P1: [4,6,7,8,6] 
		P2: 5,0,7,9,1  -> glom -> P2: [5,0,7,9,1] 



   4. mapPartitions	P: Iterable[U] -> Iterable[V]
			partition to partition transformation

		rdd1.mapPartitions(lambda p: map(lambda x: x*10, p) ).collect()
		rdd1.mapPartitions(lambda p: [sum(p)] ).collect()



   5. flatMap		P: U -> Iterable[V]
			flattens the iterables generated by the function
 			Input RDD: N objects, Output RDD: >= N objects	

		rddWords = rddFile.flatMap(lambda x: x.split())











	










 
