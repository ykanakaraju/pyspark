
  Agenda (PySpark)
  ----------------
    -> Spark - Basics & Architecture
    -> Spark Core API
	 -> RDD : Transformations & Actions
	 -> Shared Variables
    -> Spark SQL
	 -> DataFrame Operations
    -> Spark Streaming
	 -> DStreams API
	 -> Spark Structured Streaming
    -> Introduction to Spark MLlib


   Materials
   ---------
	-> PDF Presentations 
        -> Code Modules
	-> Class Notes
	=> GitHub: https://github.com/ykanakaraju/pyspark
	
   Spark
   ------
      => Open Source unified in-memory distributed computing engine/framework.
      => For big data analysis using in-memory computation on a cluster using simple programming constructs. 
      => Spark is a cluster computing framework. 

      => Spark is written in "SCALA" programming language
  
      		Cluster : Is a unified entity comprising of many nodes whose combined resources can be used to distribute
                	  storage and processing.

                => Each cluster is managed by a 'Cluster Manager'

       => Spark is a polyglot
		=> Supports Scala, Java, Python, R

       => Spark supports multiple cluster managers.
		-> local, spark standalone, YARN, Mesos, Kubernetes. 
	
      
   Spark unified stack
   --------------------
	-> Spark provides a consistent set of API for processing different analytics work loads
	   based on the same execution engine.

	 	Batch Analytics of Unstructured Data	: Spark Core (RDD API)
		Batch Analytics of Structured Data	: Spark SQL
		Streaming Analytics (real-time)		: Spark Streaming, Structured Streaming
		Predictive Analytics (ML)		: Spark MLLib
		Graph parallel computations		: Spark GraphX
	
	   => Open Source Community built APIs: spark-packages.org


   Spark Architecture
   ------------------

       1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.


    Getting Started with Spark
    --------------------------
	
    1. Working in your vLab		
	-> PySpark Shell
	-> Jupyter Notebook.

    2. Settting up pyspark environment on your personal machine.	
	-> Make sure that you install "Anaconda Navigator" 
	   URL: https://www.anaconda.com/products/distribution#windows

	-> To setup PySpark to work with Jupyter Notebook or Spyder follow the
	   instrcutions given in the shared document. 

	   https://github.com/ykanakaraju/pyspark/blob/master/Pyspark-JupyterNotebooks-Windows-Setup.pdf

    3. Signup to Databricks Community Account (free account)
	-> Sign up URL: https://databricks.com/try-databricks
	-> Login :  https://community.cloud.databricks.com/login.html
		
	Read >> Guide: Quickstart tutorial


   RDD (Resilient Distributed Dataset)
   -----------------------------------
	
      => Fundamental data abstarction of Spark Core API.

      => RDD is a collection of distributed in-memory partitions.
	  -> Partition is a colection of (in-memory) objects. 
  
      => RDDs are lazily evaluated.
	    -> Transformations does not cause execution
	    -> Action commands cause execution.

      => RDDs are immutable.

      => RDDs are resilent
	   -> RDDs can create missing in-memory partitions at run-time.
	   -> Fault-tolerant to missing in-memory partitions. 


   How to create RDD ?
   -------------------
	3 ways:

	1. Create an RDD from some external data file.
	    rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)

	2. Create an RDD from programmatic data
	    rdd1 = sc.parallelize( range(1, 1000), 3 )

	3. By applying transformations o existing RDDs
	    rdd2 = rdd1.map(lambda x: x.upper())


   What can you do with an RDD ?
   -----------------------------
	Two things:
	
	1. Transformations
		-> Create RDD Lineage DAGs
		-> Does not cause execution.

	2. Actions
		-> Trigger execution of the RDD by converting the logical plan to physical plan
		-> Produces some output.

   RDD Lineage DAG
   ---------------
    RDD Lineage DAG is a logical plan maintained by Driver for every RDD.
    RDD Lineage tracks the heirarchy of all dependent RDD all the way from the very first RDD.
    RDD Lineage DAG is created an RDD object is created by transformation or data loading commands. 

     rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)
	 => Lineage DAG of rddFile: (4)  rddFile -> textFile

     rddWords = rddFile.flatMap(lambda x: x.split(" "))
	 => Lineage DAG of rddWords: (4) rddWords -> rddFile.flatMap -> textFile

     rddPairs = rddWords.map(lambda x: (x, 1))
 	=> Lineage DAG of rddPairs: (4) rddPairs -> rddWords.map -> rddFile.flatMap -> textFile

     rddWc = rddPairs.reduceByKey(lambda x, y: x + y)
	=> Lineage DAG of rddWc: (4) rddWc -> rddPairs.reduceByKey -> rddWords.map -> rddFile.flatMap -> textFile
   	

   RDD Persistence
   ---------------

	rdd1 = sc.textFile(...)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist( StorageLevel.MEMORY_AND_DISK ) ---> instruction to Spark to save the rdd6 partitions.
	rdd7 = rdd6.t7(..)

	l1 = rdd6.collect()
	
	lineage of rdd6: rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		transformations: [sc.textFile, t3, t5, t6]

        l2 = rdd7.collect()

	lineage of rdd6: rdd7 -> rdd6.t7
		transformations: [t7]


   	Storage Levels
   	--------------	
	1. MEMORY_ONLY		: default, Memory Serialized 1x Replicated
	2. MEMORY_AND_DISK	: Disk Memory Serialized 1x Replicated
	3. DISK_ONLY		: Disk Serialized 1x Replicated
	4. MEMORY_ONLY_2	: Memory Serialized 2x Replicated (2 copies on two 'different' executors)
	5. MEMORY_AND_DISK_2	: Disk Memory Serialized 2x Replicated	


	Commands
	--------
		rdd.persist()
		rdd.persist( StorageLevel.DISK_ONLY)
		rdd.cache()

		rdd.unpersist()


   Executor's memory structure
   ----------------------------
  	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

   Types of Transformations
   ------------------------

	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD 	

   RDD Transformations
   -------------------
	
     1. map		P: U -> V
			object to object transformation
			Transformas each input object to output object by applying the function.
			input RDD: N objects, output RDD: N objects

		rddFile.map(lambda x: len(x)).collect()	


    2. filter		P: U -> Boolean
			Only those objects for which the function return True will be in the output
			input RDD: N objects, output RDD: <= N objects

		rdd1.filter(lambda x: x%3 == 0).collect()

   
    3. glom		P: None
			Returns one list object per partition with all the elements of the partition
			input RDD: N objects, output RDD: = number of partitions

		
		rdd1		    rdd2 = rdd1.glom()

		P0: 7,6,8,9,0,8 -> glom -> P0: [7,6,8,9,0,8]
		P1: 5,4,3,2,1,7 -> glom -> P1: [5,4,3,2,1,7]
		P2: 6,3,0,3,5,1	-> glom -> P2: [6,3,0,3,5,1]

		rdd1.count() = 18 (int)	  rdd2.count() = 3 (list)

		rdd1.glom().map(sum).collect()


    4. flatMap		P: U -> Iterable[V]
			Flattens the elements of the iterables produced by the function.
			input RDD: N objects, output RDD : >= N objects

		rddWords = rddFile.flatMap(lambda x: x.split(" "))


    5. mapPartitions	P: Iterable[U] -> Iterable[V]
			transforms each partition by applying the function into output partition

		rdd1          rdd2 = rdd1.mapPartitions(lambda p : [sum(p)] )  

		P0: 7,6,8,9,0,8 -> mapPartitions -> P0: 38
		P1: 5,4,3,2,1,7 -> mapPartitions -> P1: 21
		P2: 6,3,0,3,5,1	-> mapPartitions -> P2: 18

		rdd1.mapPartitions(lambda p : [ sum(p) ]).collect()
		rdd1.mapPartitions(lambda p:  map(lambda x: (x, x*10), p) ).collect()			


    6. mapPartitionsWithIndex	P: Int, Iterable[U] -> Iterable[V] 
			Similar to mapPartitions, but you get the partition-index as additional function parameter.


		rdd1.mapPartitionsWithIndex(lambda i, p : [(i, list(p))]) \
	    		.filter(lambda x: x[0] == 0) \
            		.flatMap(lambda x: x[1]).collect()


    7. distinct			P: None, Optional: numPartitions
				Returns the distinct objects of the input RDD.
				input RDD: N objects, output RDD : <= N objects

		rddWords.distinct().collect()
		rddWords.distinct(4).collect()

    Types of RDDs
    -------------
	1. Generic RDD: RDD[U]
	2. Pair RDD:	RDD[(U, V)]   


    8. mapValues	P: U -> V
			Applied only on Pair RDDs
			Transform the value part of the (k,v) by applying the function.

		rdd4.mapValues(lambda x: x[1]).collect()


    9. sortBy		P: U -> V, Optional: ascending (True/False), numPartitions
			Returns a sorted RDD where elements of the RDD are sorted based on the function output.	


		rddWords.sortBy(len).collect()				# ASC with same # of partitions
		rddWords.sortBy(len, False).collect()			# DESC with same # of partitions
		rdd1.sortBy(lambda x: x%5, True, 2).glom().collect()	# ASC with 2 partitions

    10. groupBy		P: U -> V,  Optional: numPartitions
			Returns a Pair RDD where,
			   key: unique value produced by the function
			   value: ResultIterable object with the RDD objects that produced the same key. 

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            			.flatMap(lambda x: x.split(" ")) \
            			.groupBy(lambda x: x) \
            			.mapValues(len) \
            			.sortBy(lambda x: x[1], False, 1)

    11. randomSplit	P: list of weights/ratios e.g: [0.6, 0.4], Optional: seed (any random number)
			Splits the RDD randomly into multiple RDDs in specified weights. 

		rddList = rdd1.randomSplit([0.6, 0.4])
		rddList = rdd1.randomSplit([0.6, 0.4], 575)

    12. repartition		P: numPartitions
				Is used to increase or decrease the number of partitions	
				Results in global shuffle. 

			rdd2 = rdd1.repartition(10)
			rdd3 = rdd2.repartition(5)

    13. coalesce		P: numPartitions
				Is used to decrease the number of partitions	
				Results in partition-merging
		
			rdd2 = rdd1.coalesce(5)   # here 5 should be less number than the numpartitions of rdd1

	Recommendations
        ---------------

	1. The size of each partition of the RDD should be around 128 MB
        2. The number of partitions should be a multiple of number of cores. 
        3. If the number of partitions is close to but less than 2000, bump it up to 2000
	4. The number of cores per executor should be 5


   14. partitionBy		P: numPartitions, Optional: partitioning-function (default: hash)
				Applied on Pair RDDs only
				Is used control which keys will go to which partitions.

		rddPairs.partitionBy(3, len).glom().collect()
		rddPairs.partitionBy(3).glom().collect()


   15. union, intersection, subtract, cartesian

	Let us say rdd1 has M partitions, and rdd2 has N partitions

	command			      output Partitions
	------------------------------------------------
	rdd1.union(rdd2)		M + N, narrow
	rdd1.intersection(rdd2)		M + N, wide
	rdd1.subtract(rdd2)		M + N, wide
	rdd1.cartesian(rdd2)		M * N, wide
	
   
   ..ByKey transformations
   -----------------------
	-> Are all applied to only Pair RDDs
	-> Are all wide transformations


    16. sortByKey		P: None, Optional: ascending (True/False), numPartitions		
				Sort the objects based on the key

		rdd2.sortByKey().glom().collect()
		rdd2.sortByKey(False).glom().collect()
		rdd2.sortByKey(False, 2).glom().collect()

   17. groupByKey		P: None, Optional: numPartitions
				Return a pair RDD where:
					key: unique keys of the RDD
					value: ResultIterable with all the values with the same key

				NOTE: AVOID groupByKey as much as possible.

	rdd1 => (a,1) (b,2) (c, 6) (a,11) (b,12) (c, 16) (a,21) (b,22) (c, 96) (a,31) (b,12) (c, 6)
	rdd1.groupByKey() => (a, [1,11,21,31]) (b, [2,12,22,12]) (c, [6,16,96,6])

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            			.flatMap(lambda x: x.split(" ")) \
            			.map(lambda x: (x, 1)) \
            			.groupByKey() \
            			.mapValues(sum)

   18. reduceByKey		P: U, U -> U, Optional: numPartitions
				Reduces all the values of each unique key in the RDD by iterativly
				applying the function.
				Output: unique-keys & reduced values

		rdd1							rdd2

		P0: (a,1) (b,2) (c, 6) (a,2) (b,1) (c, 6) (a,2)		P0: (a, 25)
		=> reduceByKey: (a, 5) (b, 3) (c, 12)

		P1: (a,5) (b,3) (c, 6) (a,4) (b,9) (c, 1) (a,3)		P1: (b, 25)
		=> reduceByKey: (a, 12) (b, 12) (c, 4)

		P2: (a,7) (b,6) (c, 2) (a,0) (b,4) (c, 6) (a, 1)	P2: (c, 24)
		=> reduceByKey: (a, 8) (b, 10) (c, 8)

		rdd2 = rdd1.reduceByKey(lambda x, y: x + y)

		rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4) \
            			.flatMap(lambda x: x.split(" ")) \
            			.map(lambda x: (x, 1)) \
            			.reduceByKey(lambda x, y: x + y)

   19. aggregateByKey		Is used to aggregate all the values of each unique key to a type
				different that the values of (k,v) pairs.		
				-> Applied on only pair RDDs.

		Three parameters:

		1. zero-value : Is the initial value of the type of final output.

		2. sequence-function: Merges all the values of each unique key with the zero-value
				      This is applied to every partition

		3. combine-function: Reduces all the different aggregated values of each unique-key
				     across partitions. 

		Fourth optional parameter:  numPartitions.

		student_rdd = sc.parallelize([
  			("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  			("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  			("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  			("Keerthana", "Maths", 87), ("Keerthana", "Physics", 93), ("Keerthana", "Chemistry", 91), ("Keerthana", "English", 74), 
  			("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  			("Vidya", "Maths", 86), ("Vidya", "Physics", 62), ("Vidya", "Chemistry", 75), ("Vidya", "English", 83), 
  			("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60)], 3)
 
		student_rdd.collect()

		output_rdd = student_rdd.map(lambda t : (t[0], t[2])) \
              		.aggregateByKey( (0,0),
                              lambda z, v: (z[0] + v, z[1] + 1),
                              lambda a, b: (a[0] + b[0], a[1] + b[1]),
                              2) \
              		.mapValues(lambda x: x[0]/x[1])

	
   20. join, leftOuterJoin, rightOuterJoin, fullOuterJoin    (RDD joins)

		RDD[(K, V)].join( RDD[(K, W)]) => RDD[(K, (V,W))]

		join = names1.join(names2)   #inner Join
		leftOuterJoin = names1.leftOuterJoin(names2)
		rightOuterJoin = names1.rightOuterJoin(names2)
		fullOuterJoin = names1.fullOuterJoin(names2)

   21. cogroup : groupByKey + fullOuterJoin

		[('key1', 10), ('key2', 12), ('key1', 7), ('key2', 6), ('key3', 6)]	
		=> (key1, [10, 7]) (key2, [12, 6]) (key3, [6])

		[('key1', 5), ('key2', 4), ('key2', 7), ('key1', 17), ('key4', 17)]
		=> (key1, [5, 17]) (key2, [4,7]) (key4, [17])

         cogroup: [(key1, ([10, 7],[5, 17])), (key2, ([12, 6],[4,7])), (key3, ([6], [])), (key4, ([], [17])) ]
		
	


  RDD Actions
  ------------

  1. collect

  2. count

  3. saveAsTextFile

  4. reduce		P: (U, U) -> U
			Reduces the entire RDD to one final value of the same type
			Iterativly applies the function on each partition first and across partitions.

		rdd1				

		P0: 4, 3, 2, 5, 4, 6	   -> reduce -> -16 => 52
		P1: 7, 8, 9, 0, 8, 9	   -> reduce -> -27
		P2: 0, 4, 5, 7, 8, 9, 0, 8 -> reduce -> -41

		rdd1.reduce(lambda x, y: x - y)

		rddWc.reduce(lambda x, y: (x[0] + "," + y[0], x[1] + y[1]) )

   5. aggregate
	
	Three Parameters:

		1. zero-value  : initial value of the type of the final output

		2. Sequence-function : is a function that is used to merge all the values
		   of each parititon with the zero-value. This function is applied for each
		   partition (narrow). 

               3. Combine-function : Is a reduce function that reduces all the values per 
	          partition produced by sequence function into one final value of the type
                  of the zero-value.

	rdd1					output: (300, 15)
							
	P0:  10, 10, 10, 10, 60  => (100, 5)			
	=> SF: (0,0) -> (10, 1) -> (20, 2) -> (30, 3) -> (40, 4) -> (100, 5)

	P1:  20, 10, 10, 30, 30  => (100, 5)	
	=> SF: (0,0) -> (20, 1) -> (30, 2) -> (40, 3) -> (70, 4) -> (100, 5)

	P2:  20, 10, 20, 10, 40  => (100, 5)	
	=> SF: (0,0) -> (20, 1) -> (30, 2) -> (50, 3) -> (60, 4) -> (100, 5)


	rdd1.aggregate((0, 0), 
			lambda zv, e : (zv[0]+e, zv[1]+1), 
			lambda a, b : (a[0] + b[0], a[1] + b[1]))	


	rdd1.aggregate( (0,0,0), 
			lambda z, v: (z[0]+v, z[1]+1, max(z[2],v)), 
			lambda a, b: (a[0]+b[0], a[1]+b[1], max(a[2],b[2])) )	


   6. take
		rdd1.take(5)

   7. takeOrdered
		rddWords.takeOrdered(50)
		rddWords.takeOrdered(50, len)

   8. takeSample

		rdd1.takeSample(True, 15)	# with-replacement sampling
		rdd1.takeSample(True, 15, 464)	# with-replacement sampling with seed
		rdd1.takeSample(True, 150, 464)	

		rdd1.takeSample(False, 7)	# with-out-replacement sampling
   9. first

   10. countByValue

   11. countByKey

   12. foreach         P: function that is applied on all the objects but does not return anything.

		rdd2.foreach(lambda x: print('Key: ' + str(x[0]) + ', Value:' + str(x[1])))

   13. saveAsSequenceFile


  Use-Case : 
  ---------

	dataset: https://github.com/ykanakaraju/pyspark/blob/master/data_git/cars.tsv

	From cars.tsv dataset, fetch the average-weight of each make of American origin cars.
	Arrange the data in the DESC order of average weight.
	Save the output as a single text file. 
	
         => Please try it yourself


    Closure
    -------

	A Closure constitute all the code (variables and methods) that must be visible for an executor
        to perform its computations on the RDDs. 
	
	=> The driver serializes the closure and a separate copy is sent to every executor. 


		c = 0   

		def isPrime(n):
		        TRUE if n is prime number
			FALSE if n is not a prime number

		def f1(n):
			global c
			if (isPrime(n)) c += 1
			return n*2

		rdd1 = sc.parallelize( range(1, 4001), 4 )
		rdd2 = rdd1.map( f1 )

		rdd2.collect()

		print(c)      # c = 0

	Limitation: We can not use closure variables to implement global counters. 
	Solutions: Use 'Accumulator' variables.
    
   
   Shared Variables
   ----------------

	Two distributed shared Variables:

	1. Accumulator variable

	2. Broacast variable


     1. Accumulator
		-> Maintained by the driver
		-> Not part of closure 9hnnse not a local copy)
		-> All tasks can add to this accumulator. 
		-> All tasks share one copy of the variable maintained at the driver side.
		-> Used to implement global counter

		c = sc.accumulator(0)   

		def isPrime(n):
		        TRUE if n is prime number
			FALSE if n is not a prime number

		def f1(n):
			global c
			if (isPrime(n)) c.add(1)			
			return n*2

		rdd1 = sc.parallelize( range(1, 4001), 4 )
		rdd2 = rdd1.map( f1 )

		rdd2.collect()

		print(c)   


      2. Broadcast Variable


		d = {1:a, 2:b, 3:c, 4:d, 5:e, 6;e, 7:f, .....}     # 100 MB

		def f1(n):
		   return d[n]

		rdd1 = sc.paralleize([1,2,3,4,..], 4)

		rdd2 = rdd1.map( f1 )

		rdd2.collect()    # output => [a,b,c,...]

   ===========================================================================

    spark-submit
    ------------

	=> Is a single command to submit any spark application (scala, java, python, R)
	   to any cluster manager (local, standalone, YARN, Mesos, Kubernetes)


	spark-submit [options] <app jar | python file | R file> [app arguments]

	spark-submit --master yarn 
		--deploy-mode cluster 
		--driver-memory 2G
		--driver-cores 4
		--executor-memory 10G
		--executor-cores 5
		--num-executors 10
		E:\PySpark\wordcount.py [app args]



	spark-submit --master local E:\PySpark\spark_core\examples\wordcount_cmdargs.py wordcount.txt sparkout 2
	spark-submit --master local E:\PySpark\spark_core\examples\wordcount.py


  ============================================
     Spark SQL  (pyspark.sql)
  ============================================

    => Spark's structured data processsing API

	    Structured file formats : Parquet (default), ORC, JSON, CSV (delimited text file)
	    JDBC format : RDBMS, NoSQL
	    Hive Format : Data Warehosuing platform on Hadoop

     => SparkSession
	
           -> Represents a user-session inside an application
	   -> We can have multiple sessions (SparkSession) within an application (SparkContext)
	   -> Introduced from Spark 2.x onwards (before that we has sqlContext) 

   	spark = SparkSession \
        	.builder \
        	.appName("Dataframe Operations") \
        	.config("spark.master", "local[*]") \
        	.getOrCreate()  

     => DataFrame (DF)

	  -> Data abstraction of Spark SQL
   	  -> Is a collection of distributed in-memory partitions that are immutable and lazily evaluated. 
	  -> DataFrame is a collection of 'Row' objects
		-> Row is a collection of Columns.

	  -> DataFrame has two components:
		-> Data   : Row objects
		-> Schema : StructType object 

		StructType(
		    List(
			StructField(age,LongType,true),
			StructField(gender,StringType,true),
			StructField(name,StringType,true),
			StructField(phone,StringType,true),
			StructField(userid,LongType,true)
		    )
		)


    Steps in a Spark SQL program
    ----------------------------

	1. Read/load the data from some data source to a DataFrame

		inputPath = "E:\\PySpark\\data\\users.json"
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)


        2. Transform the DF using DF API methods or using SQL

		Using DF Transformation methods
		--------------------------------
		df2 = df1.select("userid", "name", "age", "gender") \
        		.where("age is not null") \
        		.orderBy("gender", "age") \
        		.groupBy("age").count() \
        		.limit(4)

		Using SQL
		---------
		spark.catalog.listTables()
		df1.createOrReplaceTempView("users")

		df3 = spark.sql("""select age, count(*) as count
         			from users
         			where age is not null
        			group by age
         			order by age
         			limit 4""")
		df3.show()

   
        3. Write/save to some structured destination

		df2.write.format("json").save(outputPath)
		df2.write.save(outputPath, format="json")
		df2.write.json(outputPath)

  Save Modes
  -----------	
	-> ignore
	-> append
	-> overwrite

	df2.write.json(outputPath, mode="overwrite")
	df2.write.mode("overwrite").json(outputPath)	


  LocalTempView & GLobalTempView
  ------------------------------
	LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessble only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"

  DataFrame Transformations
  -------------------------

   1. select

		df2 = df1.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME", "count")

		df2 = df1.select(col("DEST_COUNTRY_NAME"),
                 		column("ORIGIN_COUNTRY_NAME"),
                 		expr("count"),
                 		df1["DEST_COUNTRY_NAME"],
                 		df1.DEST_COUNTRY_NAME)

		df2 = df1.select(col("DEST_COUNTRY_NAME").alias("destination"),
                 		col("ORIGIN_COUNTRY_NAME").alias("origin"),
                 		expr("count").cast("int"),
                 		expr("count + 10 as newCount"),
                 		expr("count > 200 as highFrequency"),
                 		expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic"))

   2. where / filter

		df3 = df2.where("domestic = false and count > 500")
		df3 = df2.filter("domestic = false and count > 500")

		df3 = df2.where(col("count") > 200)

   3. orderBy / sort

		df3 = df2.orderBy("count", "destination")
		df3 = df2.orderBy(desc("count"), asc("destination"))

		df3 = df2.sort(desc("count"), asc("destination"))		

   4. groupBy   => returns a "GroupedData" object. Apply some aggregation method to return a DF

		df3 = df2.groupBy("domestic", "highFrequency").count()
		df3 = df2.groupBy("domestic", "highFrequency").avg("count")
		df3 = df2.groupBy("domestic", "highFrequency").sum("count")
		df3 = df2.groupBy("domestic", "highFrequency").max("count")

		df3 = df2.groupBy("domestic", "highFrequency") \
         		.agg( sum("count").alias("sum"),
              			count("count").alias("count"),
              			round(avg("count"), 2).alias("avg"),
              			max("count").alias("max")
			    )

   5. limit
		df3 = df2.limit(20)

   6. selectExpr
		df2 = df1.selectExpr("DEST_COUNTRY_NAME as destination",
                 			"ORIGIN_COUNTRY_NAME as origin",
                 			"count",
                 			"count + 10 as newCount",
                 			"count > 200 as highFrequency",
                 			"DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")

		is same as:	

		df2 = df1.select(expr("DEST_COUNTRY_NAME as destination"),
                 		expr("ORIGIN_COUNTRY_NAME as origin"),
                 		expr("count"),
                 		expr("count + 10 as newCount"),
                 		expr("count > 200 as highFrequency"),
                 		expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic"))

    7. withColumn & withColumnRenamed

		df4 = df1.withColumn("newCount", col("count") + 10) \
        		.withColumn("highFrequency", expr("count > 200")) \
        		.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME")) \
        		.withColumnRenamed("DEST_COUNTRY_NAME", "destination") \
        		.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

		----------------------------------------------------------------
		listUsers = [(1, "Raju", 5),
             			(2, "Ramesh", 15),
             			(3, "Rajesh", 18),
             			(4, "Raghu", 35),
             			(5, "Ramya", 25),
             			(6, "Radhika", 35),
             			(7, "Ravi", 70)]


		df5 = spark.createDataFrame(listUsers, ["id", "name", "age"])
		df5.show()

		df6 = df5.withColumn("ageGroup", when(df5["age"] <= 12, "child")
                                 .when(df5["age"] <= 19, "teenager")
                                 .when(df5["age"] <= 60, "adult")
                                 .otherwise("senior"))

    8. udf  (user-defined-function)

	def getAgeGroup( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"
    
	getAgeGroupUDF = udf(getAgeGroup, StringType())
    
	df6 = df5.withColumn("ageGroup", getAgeGroupUDF(col("age")))

        ------------------------------------------------------------
	@udf(returnType=StringType())
	def getAgeGroup( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"
    
	df6 = df5.withColumn("ageGroup", getAgeGroup(col("age")))

	------------------------------------------------------------
	def getAgeGroup( age ):
    		if (age <= 12):
        		return "child"
    		elif (age >= 13 and age <= 19):
        		return "teenager"
    		elif (age >= 20 and age < 60):
        		return "adult"
    		else:
        		return "senior"

	spark.udf.register("get_age_group", getAgeGroup, StringType())

	qry = "select id, name, age, get_age_group(age) as ageGroup from users"
	spark.sql(qry).show()


    9. drop	=> excludes a specified columns

		df5 = df4.drop("newCount", "highFrequency")

    10. dropna	  => drops 'rows' with null values

		usersDf = spark.read.json("E:\\PySpark\\data\\users.json")
		usersDf.show()

		usersDf.dropna().show()
		usersDf.dropna(subset=["phone", "age"]).show()

    11. dropDuplicates

		listUsers = [(1, "Raju", 5),
             		(1, "Raju", 5),
             		(3, "Raju", 5),
             		(4, "Raghu", 35),
             		(4, "Raghu", 35),
             		(6, "Raghu", 35),
             		(7, "Ravi", 35)]

		userDf = spark.createDataFrame(listUsers, ["id", "name", "age"])
		userDf.show()

		userDf.dropDuplicates().show()
		userDf.dropDuplicates(["phone", "age"]).show()
		userDf.dropDuplicates(["age"]).show()

    12. distinct

		df1.select("ORIGIN_COUNTRY_NAME").distinct().count()

    13. union, intersect, subtract

		df2 = df1.where("count > 1000")
		df2.show()
		df2.count()   # 14 rows, count > 1000
		df2.rdd.getNumPartitions()

		df3 = df1.where("DEST_COUNTRY_NAME = 'India'")
		df3.show()   # 1 rows, DEST_COUNTRY_NAME = 'India'
		df3.rdd.getNumPartitions()


		df4 = df2.union(df3)
		df4 = df4.intersect(df3)
		df4 = df4.subtract(df3)

    14. sample

		df2 = df1.sample(True, 0.5)
		df2 = df1.sample(True, 0.5, 3242)
		df2 = df1.sample(True, 1.5, 3242)  # a fraction > 1 is allowed in with-replacement 

		df2 = df1.sample(False, 0.5, 3242)
		df2 = df1.sample(False, 1.5, 3242) #error: a fraction > 1 is NOT allowed in without-replacement 

    15. randomSplit

		df2, df3 = df1.randomSplit([0.6, 0.4], 445)

		df2.count()
		df3.count()

    16. repartition

		df2 = df1.repartition(6)
		df2.rdd.getNumPartitions()

		df3 = df2.repartition(3)
		df3.rdd.getNumPartitions()

		df4 = df2.repartition(3, col("DEST_COUNTRY_NAME"))
		df4.rdd.getNumPartitions()

		df5 = df2.repartition(col("DEST_COUNTRY_NAME"))
		df5.rdd.getNumPartitions()

		  => The numPartitions in the above case is determined by the value of
   		     "spark.sql.shuffle.partitions" config property (default: 200)

               	     spark.conf.set("spark.sql.shuffle.partitions", "5")

    17. coalesce

		df4 = df2.coalesce(2)
		df4.rdd.getNumPartitions()

    18. join	-> is discussed separatly.

		joinCol = employee["deptid"] == department["id"]
		joinedDf = employee.join(department, joinCol, "left_anti")

		joinedDf.show()

  show command
  -------------
	df1.show()  		# shows first 20 rows
	df1.show(50)  		# shows first 50 rows
	df1.show(50, False)	# shows first 50 rows with truncating the columns
	df1.show(5, True, True) # shows first 5 rows in verticle format


  Working with different file formats
  -----------------------------------

     JSON
     ----
	 read
		df1 = spark.read.format("json").load(inputPath)
		df1 = spark.read.load(inputPath, format="json")
		df1 = spark.read.json(inputPath)

	 write
		df2.write.format("json").save(outputPath)
		df2.write.save(outputPath, format="json")
		df2.write.json(outputPath)

     Parquet (default)
     -----------------
	 read
		df1 = spark.read.format("parquet").load(inputPath)
		df1 = spark.read.load(inputPath, format="parquet")
		df1 = spark.read.parquet(inputPath)

	 write
		df2.write.format("parquet").save(outputPath)
		df2.write.save(outputPath, format="parquet")
		df2.write.parquet(outputPath)


     ORC
     ----
	 read
		df1 = spark.read.format("orc").load(inputPath)
		df1 = spark.read.load(inputPath, format="orc")
		df1 = spark.read.orc(inputPath)

	 write
		df2.write.format("orc").save(outputPath)
		df2.write.save(outputPath, format="orc")
		df2.write.orc(outputPath)	


     CSV (delimited text format)
     ---------------------------
	read
		df1 = spark.read.option("header", True).option("inferSchema", True).csv(inputPath)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True)
		df1 = spark.read.csv(inputPath, header=True, inferSchema=True, sep="|")

	write
		df2.write.csv(outputPath, header=True, mode="overwrite")
		df2.write.csv(outputPath, header=True, mode="overwrite", sep="|")
		df2.write.option("header", True).option("sep", "|").save(inputPath)

 
     Creating an RDD from a DataFrame
     --------------------------------
	rdd1 = df1.rdd


     Creating a DataFrame from Python collections
     ---------------------------------------------
	listUsers = [(1, "Raju", 5),
             (2, "Ramesh", 15),
             (3, "Rajesh", 18),
             (4, "Raghu", 35),
             (5, "Ramya", 25),
             (6, "Radhika", 35),
             (7, "Ravi", 70)]

	df1 = spark.createDataFrame(listUsers, ["id", "name", "age"])
	df1 = spark.createDataFrame(listUsers).toDF("id", "name", "age")


     Creating a DataFrame from RDD
     ------------------------------
	rdd1 = spark.sparkContext.parallelize(listUsers)

	df2 = rdd1.toDF()
	df2 = rdd1.toDF(["id", "name", "age"])

	#df1 = spark.createDataFrame(rdd1, ["id", "name", "age"])
	#df1 = spark.createDataFrame(rdd1).toDF("id", "name", "age")

	
     Creating a DataFrame with custom schema
     ----------------------------------------
   
	mySchema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("age", IntegerType(), True)
        ])

	df2 = rdd1.toDF(schema=mySchema)
	df2.show()

	df1 = spark.createDataFrame(rdd1, schema=mySchema)
	
	--------------------------------------------------

	mySchema = StructType([
            StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
            StructField("DEST_COUNTRY_NAME", StringType(), True),
            StructField("count", IntegerType(), True)
        ])

	df1 = spark.read.json(inputPath, schema=mySchema)
	df1 = spark.read.schema(mySchema).json(inputPath)



     Joins
     ------

	Supported Joins:  
	=> inner (default), left/left_outer, right/right_outer, full/full_outer, left_semi, left_anti
    
     
         left_semi join
		=> Similar to inner join but data comes only from left-side table
		=> Equivalent to the following sub-query

		select * from emp where deptid in (select id from dept)

	left_anti join
		=> Equivalent to the following sub-query

		select * from emp where deptid not in (select id from dept)


	employee = spark.createDataFrame([
    		(1, "Raju", 25, 101),
    		(2, "Ramesh", 26, 101),
    		(3, "Amrita", 30, 102),
    		(4, "Madhu", 32, 102),
    		(5, "Aditya", 28, 102),
    		(6, "Pranav", 28, 100)])\
  		.toDF("id", "name", "age", "deptid")
  
	employee.printSchema()
	employee.show()  
  
	department = spark.createDataFrame([
    		(101, "IT", 1),
    		(102, "ITES", 1),
    		(103, "Opearation", 1),
    		(104, "HRD", 2)])\
  		.toDF("id", "deptname", "locationid")
  
	department.show()  
	department.printSchema()

        SQL Approach
        -------------

	employee.createOrReplaceTempView("emp")
	department.createOrReplaceTempView("dept")

	spark.catalog.listTables()

	spark.catalog.dropTempView("users")

	qry = """select emp.*
         	from emp left anti join dept 
         	on emp.deptid = dept.id"""

	joinedDf = spark.sql(qry)


	DF API approach
	----------------
	joinCol = employee["deptid"] == department["id"]
	joinedDf = employee.join(department, joinCol, "left_anti")

	joinedDf.show()


	Explicitly enforcing a broadcast join
        -------------------------------------
	joinedDf = employee.join(broadcast(department), joinCol, "inner")


   Use-Case
   ---------

     Dataset: https://github.com/ykanakaraju/pyspark/tree/master/data_git/movielens
    
     From moves.csv and ratings.csv datasets fetch the top 10 movies with highest average user rating
     -> The ratingCount should be atleast 30
     -> Data: movieId, title, ratingCount, averageUserRating
     -> Arrange in the DESC order of averageUserRating
     -> Save the data as a single CSV file with pipe as separator with header.

	=> Try to do it yourself.


   Window Functions
   ----------------
    
	id	department	salary	 sum("salary")
	---------------------------------------------	
	11	Sales		45000	95000
	1	Sales		50000	160000
	6	Sales		65000	?
	7	Sales		75000	

	9	HR		50000	 
	3	HR		55000	
	5	HR		55000   
	
	10	IT		40000	
	13	IT		55000	
	2	IT		60000	
	4	IT		60000	
	8	IT		60000	
	12	IT		70000	
	14	IT		80000	


	windowSpec = Window.partionBy("department") \
			   .orderBy("salary") \
			   .rowsBetween(Window.currentRow-1, Window.currentRow+1)

	#Window.unboundedPreceding, Window.unboundedFollowing, Window.currentRow

	sum("salary").over(windowSpec)

       =============================

windowSpec = Window \
  .partitionBy("CustomerId", "date") \
  .orderBy("Quantity") \
  .rowsBetween(Window.unboundedPreceding, Window.currentRow)
     
maxPurchaseQuantity = max(col("Quantity")).over(windowSpec)
avgPurchaseQuantity = round(avg(col("Quantity")).over(windowSpec), 2)
purchaseDenseRank = dense_rank().over(windowSpec)
purchaseRank = rank().over(windowSpec)
rowNumber = row_number().over(windowSpec)

dfWithDate.where("CustomerId IS NOT NULL").orderBy("CustomerId")\
  .select(
    col("CustomerId"),
    col("date"),
    col("Quantity"),
    rowNumber.alias("rowNumber"),
    purchaseRank.alias("qtyRank"),
    purchaseDenseRank.alias("qtyDenseRank"),
    avgPurchaseQuantity.alias("avgQty"),
    maxPurchaseQuantity.alias("maxQty")).show(500)

   =======================================

   JDBC Format - MySQL Integration
   -------------------------------

import os
import sys

# setup the environment for the IDE
os.environ['SPARK_HOME'] = '/home/kanak/spark-2.4.7-bin-hadoop2.7'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'

sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python')
sys.path.append('/home/kanak/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip')

from pyspark.sql import SparkSession

spark = SparkSession.builder \
            .appName("JDBC_MySQL") \
            .config('spark.master', 'local') \
            .getOrCreate()
  
qry = "(select emp.*, dept.deptname from emp left outer join dept on emp.deptid = dept.deptid) emp"
                  
df_mysql = spark.read \
            .format("jdbc") \
            .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
            .option("driver", "com.mysql.jdbc.Driver") \
            .option("dbtable", qry)  \
            .option("user", "root") \
            .option("password", "kanakaraju") \
            .load()
                
df_mysql.show()  

spark.catalog.listTables()

df2 = df_mysql.filter("age > 30").select("id", "name", "age", "deptid")

df2.show()   

df2.printSchema()    

df2.write.format("jdbc") \
    .option("url", "jdbc:mysql://localhost/sparkdb?autoReConnect=true&useSSL=false") \
    .option("driver", "com.mysql.jdbc.Driver") \
    .option("dbtable", "emp2")  \
    .option("user", "root") \
    .option("password", "kanakaraju") \
    .mode("overwrite") \
    .save()      
                                     
spark.stop()


    Hive Format - Integrating with Hive
    -----------------------------------
    
     Hive : Data Warehousing Platform on Hadoop
	
	Hive:
	   -> Warehouse : directory where Hive stores it databases & tables etc
	   -> MetaStore : is a service where Hive stores its meta-store


# -*- coding: utf-8 -*-
import findspark
findspark.init()

from os.path import abspath
from pyspark.sql import SparkSession
from pyspark.sql.functions import desc, avg, count

warehouse_location = abspath('spark-warehouse')

spark = SparkSession \
    .builder \
    .appName("Datasorces") \
    .config("spark.master", "local") \
    .config("spark.sql.warehouse.dir", warehouse_location) \
    .enableHiveSupport() \
    .getOrCreate()
    
spark.catalog.listTables()    

spark.sql("show databases").show()

spark.sql("drop database if exists sparkdemo cascade")
spark.sql("create database if not exists sparkdemo")

spark.sql("use sparkdemo")

spark.catalog.listTables()

spark.sql("DROP TABLE IF EXISTS movies")
spark.sql("DROP TABLE IF EXISTS ratings")
spark.sql("DROP TABLE IF EXISTS topRatedMovies")
    
createMovies = """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadMovies = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
createRatings = """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
loadRatings = """LOAD DATA LOCAL INPATH 'E:/PySpark/data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
         
spark.sql(createMovies)
spark.sql(loadMovies)
spark.sql(createRatings)
spark.sql(loadRatings)
    
spark.catalog.listTables()
     
#Queries are expressed in HiveQL

moviesDF = spark.sql("SELECT * FROM movies")
ratingsDF = spark.sql("SELECT * FROM ratings")

moviesDF.show()
ratingsDF.show()
           
summaryDf = ratingsDF \
            .groupBy("movieId") \
            .agg(count("rating").alias("ratingCount"), avg("rating").alias("ratingAvg")) \
            .filter("ratingCount > 25") \
            .orderBy(desc("ratingAvg")) \
            .limit(10)
              
summaryDf.show()
    
joinCol = summaryDf["movieId"] == moviesDF["movieId"]
    
summaryDf2 = summaryDf.join(moviesDF, joinCol) \
                .drop(summaryDf["movieId"]) \
                .select("movieId", "title", "ratingCount", "ratingAvg") \
                .orderBy(desc("ratingAvg")) \
                .coalesce(1)
    
summaryDf2.show()
  
summaryDf2.write \
    .mode("overwrite") \
    .format("hive") \
    .saveAsTable("topRatedMovies")

summaryDf2.printSchema()

spark.catalog.listTables()
        
topRatedMovies = spark.sql("SELECT * FROM topRatedMovies")
topRatedMovies.show() 
    
spark.catalog.listFunctions()  
  
spark.stop()

 =================================================   
     Spark Streaming
 =================================================
    
     => Spark's real-time data analytics API
   
    Two libraries
	1. Spark Streaming
	2. Structured Streaming (this is preferred)


    Spark Streaming
    ----------------

      StreamingContext :
	  -> Is the starting point of execution
	  -> Defines a micro-batch window
	  -> Each micro-batch is an RDD

       DStream (discretized stream)
	  -> Is a continuous flow of RDDs.
	  -> Each micro-batch is represented as an RDD.

	  
  	# Create a local StreamingContext with two threads and batch interval of 1 sec.
	sc = SparkContext("local[2]", "NetworkWordCount")
	ssc = StreamingContext(sc, 1)

	lines = ssc.socketTextStream("localhost", 9999)
	words = lines.flatMap(lambda line: line.split(" "))
	pairs = words.map(lambda word: (word, 1))
	wordCounts = pairs.reduceByKey(lambda x, y: x + y)
	wordCounts.print()

	ssc.start()             
	ssc.awaitTermination() 


   Spark Structured Streaming
   -------------------------- 

	-> Consider the input data stream as the 'Input Table'. 
	  Every data item that is arriving on the stream is like a new row being appended to the Input Table.

	=> DataFrame -> Unbounded Table

	-> A query on the input will generate the 'Result Table'. 

	-> Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	   which eventually updates the Result Table. 

	-> Whenever the result table gets updated, we would want to write the changed result 
	   rows to an external sink.

	Output Modes
	------------
	The 'Output' is defined as what gets written out to the external storage. 
	The output can be defined in a different mode:

	* Complete Mode - The entire updated Result Table will be written to the external storage.

	* Append Mode - Only the new rows appended in the Result Table since the last trigger will 
		      be written to the external storage. 

		      This is applicable only on the queries where existing rows in the 
		      Result Table are not expected to change.

	* Update Mode - Only the rows that were updated in the Result Table since the last trigger 
		      will be written to the external storage. 

		      Note that this is different from the Complete Mode in that this mode 
		      only outputs the rows that have changed since the last trigger. 

		      If the query doesn't contain aggregations, it will be equivalent to Append mode.


	Sample Socket Stream Example
	----------------------------

	from pyspark.sql import SparkSession
	from pyspark.sql.functions import explode
	from pyspark.sql.functions import split

	spark = SparkSession \
    		.builder \
    		.appName("StructuredNetworkWordCount") \
    		.getOrCreate()

	lines = spark \
    		.readStream \
    		.format("socket") \
    		.option("host", "localhost") \
    		.option("port", 9999) \
    		.load()

	# Split the lines into words
	words = lines.select( explode(split(lines.value, " ")).alias("word"))

	# Generate running word count
	wordCounts = words.groupBy("word").count()

	query = wordCounts \
    		.writeStream \
    		.outputMode("complete") \
    		.format("console") \
    		.start()

	query.awaitTermination() 


	=> Sources: File, Socket, Rate, Kafka

	=> Sinks: File, Console, Memory, Kafka, ForEach, ForEachBatch




  































	 
